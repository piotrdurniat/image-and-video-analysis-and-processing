{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d053f6b9",
   "metadata": {},
   "source": [
    "## Laboratorium 7\n",
    "\n",
    "\n",
    "## Detekcja obiektów za pomocą Faster-RCNN\n",
    "\n",
    "### Wprowadzenie\n",
    "\n",
    "Celem tej listy jest praktyczne zapoznanie się z działaniem dwuetapowych modeli do detekcji obiektów na przykładzie Faster R-CNN. Skorzystamy z gotowej implementacji modelu z pakietu [`torchvision`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py). Jeżeli masz inny ulubiony model działający na podobnej zasadzie, możesz z niego skorzystać zamiast podanego. Podobnie implementacja - jeśli masz swoją ulubioną bibliotekę np. Detectron2, MMDetection, możesz z niej skorzystać.\n",
    "\n",
    "W zadaniu wykorzystany zostanie zbiór danych [_Chess Pieces Dataset_](https://public.roboflow.com/object-detection/chess-full) (autorstwa Roboflow, domena publiczna), ZIP z obrazami i anotacjami powinien być dołączony do instrukcji.\n",
    "\n",
    "Podczas realizacji tej listy większy nacisk położony zostanie na inferencję z użyciem Faster R-CNN niż na uczenie (które przeprowadzisz raz\\*). Kluczowe komponenty w tej architekturze (RPN i RoIHeads) można konfigurować bez ponownego uczenia, dlatego badania skupią się na ich strojeniu. Aby zrozumieć działanie modelu, konieczne będzie spojrzenie w jego głąb, włącznie z częściowym wykonaniem. W tym celu warto mieć na podorędziu kod źródłowy, w szczególności implementacje następujących klas (uwaga - linki do najnowszej implementacji; upewnij się więc, że czytasz kod używanej przez siebie wersji biblioteki):\n",
    "* `FasterRCNN`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py\n",
    "* `GeneralizedRCNN`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/generalized_rcnn.py\n",
    "* `RegionProposalNetwork`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/rpn.py\n",
    "* `RoIHeads`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/roi_heads.py\n",
    "\n",
    "Dogłębne zrozumienie procedury uczenia modelu nie będzie wymagane, niemniej należy mieć ogólną świadomość jak ten proces przebiega i jakie funkcje kosztu są wykorzystywane. Użyjemy gotowej implementacji z submodułu [`references.detection`](https://github.com/pytorch/vision/blob/main/references/detection/train.py) w nieco uproszczonej wersji. Ponieważ ten moduł **nie** jest domyślnie instalowaną częścią pakietu `torchvision`, do instrukcji dołączono jego kod w nieznacznie zmodyfikowanej wersji (`references_detection.zip`).\n",
    "Jeśli ciekawią Cię szczegóły procesu uczenia, zachęcam do lektury [artykułu](https://arxiv.org/abs/1506.01497) i analizy kodu implementacji."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4991badb",
   "metadata": {},
   "source": [
    "!pip install torchvision>=0.13 # jeśli nie posiadasz pakietu torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ca0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886028c4",
   "metadata": {},
   "source": [
    "### Zadanie 0: Uczenie\n",
    "\n",
    "Krokiem \"zerowym\" będzie przygotowanie wstępnie nauczonego modelu i douczenie go na docelowym zbiorze.\n",
    "Podany zestaw hiperparametrów powinien dawać przyzwoite (niekoniecznie idealne) wyniki - jeśli chcesz, śmiało dobierz swoje własne; nie spędzaj na tym jednak zbyt wiele czasu.\n",
    "\n",
    "Twoim zadaniem jest nie tylko przeklikanie poniższych komórek, ale przynajmniej ogólne zrozumienie procesu uczenia (przejrzyj implementację `train_one_epoch`) i struktury modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37782fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection as M\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from detection import coco_utils, presets, utils, transforms\n",
    "from detection.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8dd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(img_root: str, file_name: str, train: bool = True):\n",
    "    \"\"\"Reimplementacja analogicznej funkcji z pakietu references, rozwiązująca drobną niekompatybilność w zbiorze CPD\"\"\"\n",
    "\n",
    "    def fake_segmentation(image, target):\n",
    "        for obj in target[\"annotations\"]:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            segm = [x, y, x + w, y, x + w, y + h, x, y + h]\n",
    "            obj[\"segmentation\"] = [segm]\n",
    "        return image, target\n",
    "\n",
    "    tfs = transforms.Compose(\n",
    "        [\n",
    "            fake_segmentation,\n",
    "            coco_utils.ConvertCocoPolysToMask(),\n",
    "            (\n",
    "                presets.DetectionPresetTrain(data_augmentation=\"hflip\")\n",
    "                if train\n",
    "                else presets.DetectionPresetEval()\n",
    "            ),\n",
    "            # jeśli chcesz dodać swoje własne augmentacje, możesz zrobić to tutaj\n",
    "        ]\n",
    "    )\n",
    "    ds = coco_utils.CocoDetection(img_root, file_name, transforms=tfs)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523b9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja hiperparametrów\n",
    "LR = 0.001  # powinno być dobrze dla 1 GPU\n",
    "WDECAY = 0.0001\n",
    "EPOCHS = 25\n",
    "VAL_FREQ = 5  # walidacja i checkpointowanie co N epok\n",
    "BATCH_SIZE = 2  # dobierz pod możliwości sprzętowe\n",
    "NUM_WORKERS = 8  # j/w\n",
    "NUM_CLASSES = 14\n",
    "DEVICE = \"cuda:0\"\n",
    "DATASET_ROOT = \"chess/\"\n",
    "OUTPUT_DIR = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23d777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Zaczytanie datasetów\n",
    "chess_train = get_dataset(\n",
    "    os.path.join(DATASET_ROOT, \"train\"),\n",
    "    os.path.join(DATASET_ROOT, \"train/_annotations.coco.json\"),\n",
    ")\n",
    "chess_val = get_dataset(\n",
    "    os.path.join(DATASET_ROOT, \"valid\"),\n",
    "    os.path.join(DATASET_ROOT, \"valid/_annotations.coco.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49517ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samplery i loadery\n",
    "train_sampler = torch.utils.data.RandomSampler(chess_train)\n",
    "train_batch_sampler = torch.utils.data.BatchSampler(\n",
    "    train_sampler, BATCH_SIZE, drop_last=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    chess_train,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(chess_val)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    chess_val,\n",
    "    batch_size=1,\n",
    "    sampler=val_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skonstruowanie modelu; tworzymy w wersji dla 91 klas aby zainicjować wagi wstępnie nauczone na COCO...\n",
    "model = M.fasterrcnn_resnet50_fpn(\n",
    "    weights=M.FasterRCNN_ResNet50_FPN_Weights.COCO_V1, num_classes=91\n",
    ").to(DEVICE)\n",
    "# ...po czym zastępujemy predyktor mniejszym, dostosowanym do naszego zbioru:\n",
    "model.roi_heads.box_predictor = M.faster_rcnn.FastRCNNPredictor(\n",
    "    in_channels=1024, num_classes=NUM_CLASSES\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894b7079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=14, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=56, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  # zwróć uwagę na strukturę Box Predictora (dlaczego tyle out_features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ab9690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/projects/ai/apow/lab7/detection/engine.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/101]  eta: 0:07:16  lr: 0.000011  loss: 2.9982 (2.9982)  loss_classifier: 2.8630 (2.8630)  loss_box_reg: 0.1037 (0.1037)  loss_objectness: 0.0309 (0.0309)  loss_rpn_box_reg: 0.0006 (0.0006)  time: 4.3186  data: 0.6873  max mem: 2419\n",
      "Epoch: [0]  [ 20/101]  eta: 0:04:01  lr: 0.000211  loss: 3.4107 (3.3939)  loss_classifier: 2.4854 (2.4395)  loss_box_reg: 0.4808 (0.4682)  loss_objectness: 0.4670 (0.4572)  loss_rpn_box_reg: 0.0331 (0.0290)  time: 2.9090  data: 0.0262  max mem: 2663\n",
      "Epoch: [0]  [ 40/101]  eta: 0:03:01  lr: 0.000411  loss: 1.6464 (2.5534)  loss_classifier: 0.9772 (1.6936)  loss_box_reg: 0.3666 (0.4156)  loss_objectness: 0.4248 (0.4161)  loss_rpn_box_reg: 0.0306 (0.0280)  time: 2.9851  data: 0.0244  max mem: 2663\n",
      "Epoch: [0]  [ 60/101]  eta: 0:02:02  lr: 0.000610  loss: 1.2062 (2.0816)  loss_classifier: 0.5029 (1.2989)  loss_box_reg: 0.3009 (0.3722)  loss_objectness: 0.3222 (0.3833)  loss_rpn_box_reg: 0.0266 (0.0273)  time: 2.9901  data: 0.0240  max mem: 2663\n",
      "Epoch: [0]  [ 80/101]  eta: 0:01:03  lr: 0.000810  loss: 1.0128 (1.8126)  loss_classifier: 0.3926 (1.0788)  loss_box_reg: 0.2390 (0.3396)  loss_objectness: 0.2960 (0.3673)  loss_rpn_box_reg: 0.0261 (0.0270)  time: 3.0882  data: 0.0247  max mem: 2663\n",
      "Epoch: [0]  [100/101]  eta: 0:00:03  lr: 0.001000  loss: 0.8393 (1.6424)  loss_classifier: 0.3514 (0.9370)  loss_box_reg: 0.2065 (0.3149)  loss_objectness: 0.3337 (0.3631)  loss_rpn_box_reg: 0.0265 (0.0273)  time: 3.1017  data: 0.0246  max mem: 2670\n",
      "Epoch: [0] Total time: 0:05:05 (3.0282 s / it)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<detection.utils.MetricLogger at 0x78c453106240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zanim przejdziemy do uczenia pełnego modelu, wykonamy krótkie wstępne uczenie losowo zainicjowanego predyktora:\n",
    "train_one_epoch(\n",
    "    model=model,\n",
    "    optimizer=torch.optim.AdamW(\n",
    "        model.roi_heads.box_predictor.parameters(), lr=LR, weight_decay=WDECAY\n",
    "    ),\n",
    "    data_loader=train_loader,\n",
    "    device=DEVICE,\n",
    "    epoch=0,\n",
    "    print_freq=20,\n",
    "    scaler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683b699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Zapisywanie nauczonych modeli\n",
    "model_path = \"trained_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d18b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/101]  eta: 0:08:33  lr: 0.000011  loss: 0.5060 (0.5060)  loss_classifier: 0.1822 (0.1822)  loss_box_reg: 0.2979 (0.2979)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0255 (0.0255)  time: 5.0867  data: 2.0835  max mem: 2902\n",
      "Epoch: [0]  [ 20/101]  eta: 0:04:05  lr: 0.000211  loss: 0.2124 (0.2277)  loss_classifier: 0.0564 (0.0728)  loss_box_reg: 0.1408 (0.1470)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0057 (0.0077)  time: 2.9307  data: 0.0227  max mem: 3163\n",
      "Epoch: [0]  [ 40/101]  eta: 0:03:04  lr: 0.000411  loss: 0.2282 (0.2392)  loss_classifier: 0.0678 (0.0792)  loss_box_reg: 0.1392 (0.1502)  loss_objectness: 0.0001 (0.0004)  loss_rpn_box_reg: 0.0094 (0.0093)  time: 3.0268  data: 0.0273  max mem: 3214\n",
      "Epoch: [0]  [ 60/101]  eta: 0:02:05  lr: 0.000610  loss: 0.1906 (0.2312)  loss_classifier: 0.0645 (0.0788)  loss_box_reg: 0.1147 (0.1422)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0088 (0.0093)  time: 3.1237  data: 0.0264  max mem: 3214\n",
      "Epoch: [0]  [ 80/101]  eta: 0:01:04  lr: 0.000810  loss: 0.2636 (0.2402)  loss_classifier: 0.0901 (0.0817)  loss_box_reg: 0.1499 (0.1470)  loss_objectness: 0.0003 (0.0013)  loss_rpn_box_reg: 0.0129 (0.0102)  time: 3.1378  data: 0.0268  max mem: 3214\n",
      "Epoch: [0]  [100/101]  eta: 0:00:03  lr: 0.001000  loss: 0.2608 (0.2390)  loss_classifier: 0.0804 (0.0821)  loss_box_reg: 0.1418 (0.1459)  loss_objectness: 0.0003 (0.0014)  loss_rpn_box_reg: 0.0042 (0.0097)  time: 3.1221  data: 0.0227  max mem: 3214\n",
      "Epoch: [0] Total time: 0:05:11 (3.0887 s / it)\n",
      "Epoch: [1]  [  0/101]  eta: 0:06:32  lr: 0.001000  loss: 0.0836 (0.0836)  loss_classifier: 0.0182 (0.0182)  loss_box_reg: 0.0637 (0.0637)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0017 (0.0017)  time: 3.8840  data: 0.7474  max mem: 3214\n",
      "Epoch: [1]  [ 20/101]  eta: 0:04:16  lr: 0.001000  loss: 0.3300 (0.3396)  loss_classifier: 0.1089 (0.1220)  loss_box_reg: 0.1914 (0.1932)  loss_objectness: 0.0051 (0.0075)  loss_rpn_box_reg: 0.0116 (0.0170)  time: 3.1337  data: 0.0235  max mem: 3214\n",
      "Epoch: [1]  [ 40/101]  eta: 0:03:12  lr: 0.001000  loss: 0.3958 (0.4149)  loss_classifier: 0.1439 (0.1582)  loss_box_reg: 0.2043 (0.2080)  loss_objectness: 0.0094 (0.0253)  loss_rpn_box_reg: 0.0193 (0.0235)  time: 3.1435  data: 0.0245  max mem: 3214\n",
      "Epoch: [1]  [ 60/101]  eta: 0:02:09  lr: 0.001000  loss: 0.9072 (0.7092)  loss_classifier: 0.4298 (0.3280)  loss_box_reg: 0.3317 (0.2652)  loss_objectness: 0.0371 (0.0727)  loss_rpn_box_reg: 0.0600 (0.0434)  time: 3.1470  data: 0.0246  max mem: 3214\n",
      "Epoch: [1]  [ 80/101]  eta: 0:01:06  lr: 0.001000  loss: 0.9999 (0.7920)  loss_classifier: 0.5272 (0.3799)  loss_box_reg: 0.3922 (0.3033)  loss_objectness: 0.0177 (0.0640)  loss_rpn_box_reg: 0.0435 (0.0449)  time: 3.1497  data: 0.0264  max mem: 3214\n",
      "Epoch: [1]  [100/101]  eta: 0:00:03  lr: 0.001000  loss: 0.7910 (0.8004)  loss_classifier: 0.4207 (0.3897)  loss_box_reg: 0.3060 (0.3126)  loss_objectness: 0.0081 (0.0540)  loss_rpn_box_reg: 0.0354 (0.0442)  time: 3.1588  data: 0.0265  max mem: 3216\n",
      "Epoch: [1] Total time: 0:05:18 (3.1544 s / it)\n",
      "Epoch: [2]  [  0/101]  eta: 0:08:40  lr: 0.001000  loss: 0.7192 (0.7192)  loss_classifier: 0.3554 (0.3554)  loss_box_reg: 0.3134 (0.3134)  loss_objectness: 0.0084 (0.0084)  loss_rpn_box_reg: 0.0420 (0.0420)  time: 5.1527  data: 1.9595  max mem: 3216\n",
      "Epoch: [2]  [ 20/101]  eta: 0:04:24  lr: 0.001000  loss: 0.6262 (0.7244)  loss_classifier: 0.3241 (0.3646)  loss_box_reg: 0.2867 (0.3235)  loss_objectness: 0.0022 (0.0057)  loss_rpn_box_reg: 0.0220 (0.0307)  time: 3.1657  data: 0.0268  max mem: 3216\n",
      "Epoch: [2]  [ 40/101]  eta: 0:03:15  lr: 0.001000  loss: 0.5424 (0.6221)  loss_classifier: 0.2715 (0.3126)  loss_box_reg: 0.2314 (0.2777)  loss_objectness: 0.0011 (0.0048)  loss_rpn_box_reg: 0.0225 (0.0270)  time: 3.1499  data: 0.0248  max mem: 3216\n",
      "Epoch: [2]  [ 60/101]  eta: 0:02:10  lr: 0.001000  loss: 0.4631 (0.5818)  loss_classifier: 0.1998 (0.2803)  loss_box_reg: 0.2441 (0.2689)  loss_objectness: 0.0027 (0.0061)  loss_rpn_box_reg: 0.0206 (0.0264)  time: 3.1504  data: 0.0254  max mem: 3216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# eval and checkpoint every VAL_FREQ epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ai/apow/lab7/detection/engine.py:57\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mmetric_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_dict_reduced\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     metric_logger\u001b[38;5;241m.\u001b[39mupdate(lr\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric_logger\n",
      "File \u001b[0;32m~/projects/ai/apow/lab7/detection/utils.py:121\u001b[0m, in \u001b[0;36mMetricLogger.update\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 121\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeters[k]\u001b[38;5;241m.\u001b[39mupdate(v)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Uczenie pełnego modelu\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad], lr=LR, weight_decay=WDECAY\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[10], gamma=0.1\n",
    ")  # dobierz wartości jeśli trzeba\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(model, optimizer, train_loader, DEVICE, epoch, 20, None)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # eval and checkpoint every VAL_FREQ epochs\n",
    "    if (epoch + 1) % VAL_FREQ == 0:\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        utils.save_on_master(checkpoint, os.path.join(OUTPUT_DIR, f\"model_{epoch}.pth\"))\n",
    "        utils.save_on_master(checkpoint, os.path.join(OUTPUT_DIR, \"checkpoint.pth\"))\n",
    "        evaluate(model, val_loader, device=DEVICE)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1765096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencja na zadanym obrazie\n",
    "preprocess = (\n",
    "    M.FasterRCNN_ResNet50_FPN_Weights.COCO_V1.transforms()\n",
    ")  # to wystarczy pobrać raz\n",
    "img = read_image(\n",
    "    os.path.join(\n",
    "        DATASET_ROOT, \"test/IMG_0159_JPG.rf.1cf4f243b5072d63e492711720df35f7.jpg\"\n",
    "    )\n",
    ")\n",
    "batch = [preprocess(img).to(DEVICE)]\n",
    "prediction = model(batch)[0]\n",
    "# Rysowanie predykcji - wygodny gotowiec\n",
    "box = draw_bounding_boxes(\n",
    "    img,\n",
    "    boxes=prediction[\"boxes\"],\n",
    "    labels=[chess_train.coco.cats[i.item()][\"name\"] for i in prediction[\"labels\"]],\n",
    "    colors=\"red\",\n",
    "    width=4,\n",
    ")\n",
    "to_pil_image(box.detach()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90665f",
   "metadata": {},
   "source": [
    "---\n",
    "### Zadanie 1\n",
    "\n",
    "Zbadaj wpływ parametrów inferencji **głowic `RoIHeads`**, progu prawdopodobieństwa (`score_thresh`) i progu NMS (`nms_thresh`), na działanie modelu. Wykorzystaj funkcję `evaluate` aby zmierzyć zmianę jakości predykcji, ale przebadaj też efekty wizualnie, wyświetlając predykcje dla kilku obrazów ze zbioru walidacyjnego i kilku spoza zbioru (folder `wild`). _W finalnej wersji pozostaw tylko wybrane interesujące przykłady._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f52e1",
   "metadata": {},
   "source": [
    "- `score_thresh` - próg dla minimalnej wartości pewności predykcji, powyżej której model uznaje detekcję za wartą zachowania\n",
    "    - niski próg (np. 0.1)\n",
    "        - więcej detekcji\n",
    "        - więcej false positive\n",
    "        - mniej false negative  \n",
    "        - wysoki recall\n",
    "        - niskie precision\n",
    "    - wysoki próg (np. 0.8)\n",
    "        - mniej detekcji\n",
    "        - mniej false positive\n",
    "        - więcej false negative \n",
    "- `nms_thresh` - Non-Maximum Suppression (NMS) - określa próg jak bardzo mogą nakładać się bounding boxy (wg. wartości IoU)\n",
    "    - Zawsze jedna detekcja o największym score jest zachowana\n",
    "    - Dla każdej z detekcji, oblicza IoU z pozostałymi:\n",
    "    - Usuwa detekcję, jeśli IoU > nms_thresh \n",
    "    - Niskie wartości (np. 0.3) - usuwa więcej detekcji\n",
    "    - Wysokie wartości (np. 0.7) - usuwa mniej detekcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c830b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załadowanie modelu z pliku checkpoint\n",
    "\n",
    "# Skonstruowanie modelu; tworzymy w wersji dla 91 klas aby zainicjować wagi wstępnie nauczone na COCO...\n",
    "model = M.fasterrcnn_resnet50_fpn(\n",
    "    weights=M.FasterRCNN_ResNet50_FPN_Weights.COCO_V1, num_classes=91\n",
    ").to(DEVICE)\n",
    "# ...po czym zastępujemy predyktor mniejszym, dostosowanym do naszego zbioru:\n",
    "model.roi_heads.box_predictor = M.faster_rcnn.FastRCNNPredictor(\n",
    "    in_channels=1024, num_classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "checkpoint_file = os.path.join(OUTPUT_DIR, \"checkpoint.pth\")\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46588246",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(\n",
    "    DATASET_ROOT, \"test/IMG_0159_JPG.rf.1cf4f243b5072d63e492711720df35f7.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe1eae",
   "metadata": {},
   "source": [
    "#### Get evaluation scores on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa69a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Score Thresholds:\")\n",
    "score_thresholds = [0.1, 0.5, 0.8]\n",
    "\n",
    "for score_thresh in score_thresholds:\n",
    "    model.roi_heads.score_thresh = score_thresh\n",
    "\n",
    "    print(f\"\\nScore threshold: {score_thresh}\")\n",
    "    evaluate(model, val_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTesting NMS Thresholds:\")\n",
    "nms_thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "for nms_thresh in nms_thresholds:\n",
    "    model.roi_heads.nms_thresh = nms_thresh\n",
    "\n",
    "    print(f\"\\nNMS threshold: {nms_thresh}\")\n",
    "    evaluate(model, val_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e8846",
   "metadata": {},
   "source": [
    "#### Visualize bbox predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39950ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113816fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, image_path, title=\"\") -> PIL.Image:\n",
    "    img = read_image(image_path)\n",
    "    preprocess = M.FasterRCNN_ResNet50_FPN_Weights.COCO_V1.transforms()\n",
    "    batch = [preprocess(img).to(DEVICE)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(batch)[0]\n",
    "\n",
    "    box = draw_bounding_boxes(\n",
    "        img,\n",
    "        boxes=prediction[\"boxes\"],\n",
    "        labels=[chess_train.coco.cats[i.item()][\"name\"] for i in prediction[\"labels\"]],\n",
    "        colors=\"red\",\n",
    "        width=4,\n",
    "    )\n",
    "    return to_pil_image(box.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "score_thresholds = [0.1, 0.5, 0.8]\n",
    "\n",
    "test_img = os.path.join(\n",
    "    DATASET_ROOT, \"test/IMG_0159_JPG.rf.1cf4f243b5072d63e492711720df35f7.jpg\"\n",
    ")\n",
    "\n",
    "for i, thresh in enumerate(score_thresholds):\n",
    "    model.roi_heads.score_thresh = thresh\n",
    "    pred_img = visualize_predictions(model, test_img, f\"Score threshold: {thresh}\")\n",
    "\n",
    "    # Convert PIL image to array for matplotlib\n",
    "    axes[i].imshow(pred_img)\n",
    "    axes[i].set_title(f\"Score threshold: {thresh}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e7ee7",
   "metadata": {},
   "source": [
    "### Zadanie 2a\n",
    "\n",
    "Zwizualizuj propozycje rejonów wygenerowane przez RPN i porównaj z ostateczną predykcją.\n",
    "\n",
    "W tym celu konieczne będzie manualne wykonanie fragmentu metody `GeneralizedRCNN::forward` (patrz: [kod](https://github.com/pytorch/vision/blob/6279faa88a3fe7de49bf58284d31e3941b768522/torchvision/models/detection/generalized_rcnn.py#L46), link do wersji najnowszej na grudzień 2024).\n",
    "Wszystkie fragmenty związane z uczeniem możesz rzecz jasna pominąć; chodzi o wyciągnięcie obiektu `proposals`.\n",
    "Nie zapomnij o wykonaniu powrotnej transformacji! (Po co?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89669823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebf536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5480a0dd",
   "metadata": {},
   "source": [
    "### Zadanie 2b\n",
    "\n",
    "Zbadaj wpływ progu NMS _na etapie propozycji_ na jakość predykcji oraz czas ich uzyskania.\n",
    "Jak w poprzednich zadaniach, postaraj się nie ograniczyć tylko do pokazania metryk, ale pokaż wizualizacje (propozycji i predykcji) dla **wybranych** przykładów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdd0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623dd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
